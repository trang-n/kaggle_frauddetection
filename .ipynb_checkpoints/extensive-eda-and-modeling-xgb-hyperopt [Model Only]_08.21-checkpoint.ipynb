{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Standard plotly imports\n",
    "#import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "#import cufflinks\n",
    "#import cufflinks as cf\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Using plotly + cufflinks in offline mode\n",
    "init_notebook_mode(connected=True)\n",
    "#cufflinks.go_offline(connected=True)\n",
    "\n",
    "# Preprocessing, modelling and evaluating\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "## Hyperopt modules\n",
    "from hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import gc\n",
    "# print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REducing memory\n",
    "# df_trans = reduce_mem_usage(df_trans)\n",
    "# df_id = reduce_mem_usage(df_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans = pd.read_csv('data/train_transaction.csv')\n",
    "df_test_trans = pd.read_csv('data/test_transaction.csv')\n",
    "\n",
    "df_id = pd.read_csv('data/train_identity.csv')\n",
    "df_test_id = pd.read_csv('data/test_identity.csv')\n",
    "\n",
    "sample_submission = pd.read_csv('data/sample_submission.csv', index_col='TransactionID')\n",
    "\n",
    "df_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True, on='TransactionID')\n",
    "df_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "# y_train = df_train['isFraud'].copy()\n",
    "del df_trans, df_id, df_test_trans, df_test_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reducing memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = reduce_mem_usage(df_train)\n",
    "# df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n",
    "#           'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n",
    "#           'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n",
    "#           'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n",
    "#           'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n",
    "#           'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n",
    "#           'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n",
    "#           'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n",
    "#           'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n",
    "#           'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n",
    "#           'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n",
    "#           'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n",
    "#           'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n",
    "#           'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n",
    "#           'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n",
    "#           'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n",
    "#           'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n",
    "#           'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n",
    "#           'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "\n",
    "# us_emails = ['gmail', 'net', 'edu']\n",
    "\n",
    "# # https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest-579654\n",
    "# for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "#     df_train[c + '_bin'] = df_train[c].map(emails)\n",
    "#     df_test[c + '_bin'] = df_test[c].map(emails)\n",
    "    \n",
    "#     df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1]) # find .com, ,mx ..\n",
    "#     df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "#     df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "#     df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us') \n",
    "#     # if not US,keep it as it is; if US, turn into suffix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hours and Days "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_encoder(df, offset = 0, colname = 'TransactionDT'):\n",
    "    days = df[colname] / (3600*24)\n",
    "    encode_days = np.floor(days - 1 + offset) % 7 #review\n",
    "    return encode_days\n",
    "\n",
    "def hour_encoder(df, colname = 'TransactionDT'):\n",
    "    hours = df_train['TransactionDT'] / 3600\n",
    "    encode_hours = np.floor(hours) % 24 #review\n",
    "    return encode_hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['weekday'] = day_encoder(df_train, offset = 0.58)\n",
    "df_test['weekday'] = day_encoder(df_test, offset=  0.58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['hours'] = hour_encoder(df_train)\n",
    "df_test['hours'] = hour_encoder(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "for f in df_train.drop('isFraud', axis=1).columns:\n",
    "    if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n",
    "        df_train[f] = lbl.transform(list(df_train[f].values))\n",
    "        df_test[f] = lbl.transform(list(df_test[f].values))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is normalizing \n",
    "df_train['Trans_min_mean'] = df_train['TransactionAmt'] - df_train['TransactionAmt'].mean()\n",
    "df_train['Trans_min_std'] = df_train['Trans_min_mean'] / df_train['TransactionAmt'].std()\n",
    "df_test['Trans_min_mean'] = df_test['TransactionAmt'] - df_test['TransactionAmt'].mean()\n",
    "df_test['Trans_min_std'] = df_test['Trans_min_mean'] / df_test['TransactionAmt'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train['TransactionAmt_to_mean_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "# df_train['TransactionAmt_to_mean_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "# df_train['TransactionAmt_to_std_card1'] = df_train['TransactionAmt'] / df_train.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "# df_train['TransactionAmt_to_std_card4'] = df_train['TransactionAmt'] / df_train.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "\n",
    "# df_test['TransactionAmt_to_mean_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('mean')\n",
    "# df_test['TransactionAmt_to_mean_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "# df_test['TransactionAmt_to_std_card1'] = df_test['TransactionAmt'] / df_test.groupby(['card1'])['TransactionAmt'].transform('std')\n",
    "# df_test['TransactionAmt_to_std_card4'] = df_test['TransactionAmt'] / df_test.groupby(['card4'])['TransactionAmt'].transform('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['TransactionAmt'] = np.log(df_train['TransactionAmt'])\n",
    "df_test['TransactionAmt'] = np.log(df_test['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concating dfs to get PCA of V features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['isFraud'] = 'test'\n",
    "df = pd.concat([df_train, df_test], axis=0, sort=False )\n",
    "df = df.reset_index()\n",
    "df = df.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_change(df, cols, n_components, prefix='PCA_', rand_seed=4):\n",
    "    pca = PCA(n_components=n_components, random_state=rand_seed)\n",
    "\n",
    "    principalComponents = pca.fit_transform(df[cols])\n",
    "\n",
    "    principalDf = pd.DataFrame(principalComponents)\n",
    "\n",
    "    df.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n",
    "\n",
    "    df = pd.concat([df, principalDf], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_v = df_train.columns[55:394]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "for col in mas_v:\n",
    "    df[col] = df[col].fillna((df[col].min() - 2))\n",
    "    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n",
    "\n",
    "    \n",
    "df = PCA_change(df, mas_v, prefix='PCA_V_', n_components=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seting train and test back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590540, 129)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seting X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.sort_values('TransactionDT').drop(['isFraud', \n",
    "                                                      'TransactionDT', \n",
    "                                                      #'Card_ID'\n",
    "                                                     ],\n",
    "                                                     axis=1)\n",
    "y_train = df_train.sort_values('TransactionDT')['isFraud'].astype(bool)\n",
    "\n",
    "X_test = df_test.sort_values('TransactionDT').drop(['TransactionDT',\n",
    "                                                    #'Card_ID'\n",
    "                                                   ], \n",
    "                                                   axis=1)\n",
    "del df_train\n",
    "df_test = df_test[[\"TransactionDT\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JC trying LightBGM grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "# import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search = {\n",
    "#     \"verbosity\": [-5],\n",
    "#     \"num_boost_round\": [250, 750,1500],\n",
    "#     \"max_depth\": [250, 500, 750],\n",
    "#     \"learning_rate\": np.linspace(1e-3, 1,5),\n",
    "#     \"num_leaves\": [5,10,20],\n",
    "#     \"min_child_samples\": [10,30,50],\n",
    "#     \"min_child_weight\": [1e-3],\n",
    "#     \"subsample\": np.linspace(0.1,1,5),  # subsample = bagging\n",
    "# #     \"subsample_freq\": [],\n",
    "#     \"colsample_bytree\": [0.5, 0.7,1.0]  # feature fraction: \"mtry\"\n",
    "#     #\"max_delta_step\": -1,\n",
    "# #     \"reg_alpha\": [0.05],      # L1\n",
    "# #     \"reg_lambda\": [2]  # ,   # L2\n",
    "#     # \"min_split_gain\": 0.0,\n",
    "#     # \"drop_rate\": 0.1, # dart only\n",
    "#     # \"max_drop\": 50, # dart only\n",
    "#     # \"skip_drop\": 0.5, # dart only\n",
    "#     # \"uniform_drop\": False, # dart only\n",
    "#     # \"top_rate\": 0.2, # goss only\n",
    "#     # \"other_rate\": 0.1, # goss only\n",
    "#     # \"min_data_per_group\": 100,\n",
    "#     # \"max_cat_threshold\": 32,\n",
    "#     # \"cat_l2\": 10.0,\n",
    "#     # \"cat_smooth\": 10.0,\n",
    "#     # \"max_cat_to_onehot\": 4,\n",
    "#     # \"topk\": 20, # larger -> more accurate but slow\n",
    "    \n",
    "# }\n",
    "\n",
    "# ## n_jobs sets number of cores to parallelize on, set to 4 if you have 8 cores if you are getting bottlenecked\n",
    "# ## verbose defines how much information is printed to console during search\n",
    "\n",
    "# tss = TimeSeriesSplit(n_splits=7)\n",
    "# model_grid = GridSearchCV(lgb.LGBMRegressor(\n",
    "#     objective=\"binary\", metric=\"auc\", boosting_type=\"gbdt\", device_type=\"cpu\", tree_learner=\"feature\"),\n",
    "#     search, cv=tss)\n",
    "\n",
    "# model_grid.fit(X_train,y_train)\n",
    "\n",
    "# params = model_grid.best_params_\n",
    "\n",
    "# print(f'Best grid search parameters:\\n      {params}')\n",
    "# print(f'Best grid search loss:\\n         {model_grid.best_score_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the HyperOpt function with parameters space and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from xgboost import plot_importance\n",
    "# from sklearn.metrics import make_scorer\n",
    "\n",
    "# import time\n",
    "# def objective(params, FOLDS = 7):\n",
    "#     time1 = time.time()\n",
    "#     params = {\n",
    "#         'max_depth': int(params['max_depth']),\n",
    "#         'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "#         'subsample': \"{:.2f}\".format(params['subsample']),\n",
    "#         'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n",
    "#         'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n",
    "#         'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n",
    "#         'num_leaves': '{:.3f}'.format(params['num_leaves']),\n",
    "#         'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "#         'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n",
    "#         'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n",
    "#         'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n",
    "#     }\n",
    "\n",
    "#     print(\"\\n############## New Run ################\")\n",
    "#     print(f\"params = {params}\")\n",
    "    \n",
    "#     count=1\n",
    "\n",
    "#     tss = TimeSeriesSplit(n_splits=FOLDS)\n",
    "#     y_preds = np.zeros(sample_submission.shape[0])\n",
    "#     y_oof = np.zeros(X_train.shape[0])\n",
    "#     score_mean = 0\n",
    "#     for tr_idx, val_idx in tss.split(X_train, y_train):\n",
    "#         clf = xgb.XGBClassifier(\n",
    "#             n_estimators=600, random_state=4, verbose=True,  \n",
    "#             **params\n",
    "#         )\n",
    "\n",
    "#         X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n",
    "#         y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "#         clf.fit(X_tr, y_tr)\n",
    "#         #y_pred_train = clf.predict_proba(X_vl)[:,1]\n",
    "#         #print(y_pred_train)\n",
    "#         score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n",
    "#         # plt.show()\n",
    "#         score_mean += score\n",
    "#         print(f'{count} CV - score: {round(score, 4)}')\n",
    "#         count += 1\n",
    "#     time2 = time.time() - time1\n",
    "#     print(f\"Total Time Run: {round(time2 / 60,2)}\")\n",
    "#     gc.collect()\n",
    "#     print(f'Mean ROC_AUC: {score_mean / FOLDS}')\n",
    "#     del X_tr, X_vl, y_tr, y_vl, clf, score\n",
    "#     return -(score_mean / FOLDS)\n",
    "\n",
    "\n",
    "# space = {\n",
    "#     # The maximum depth of a tree, same as GBM.\n",
    "#     # Used to control over-fitting as higher depth will allow model \n",
    "#     # to learn relations very specific to a particular sample.\n",
    "#     # Should be tuned using CV.\n",
    "#     # Typical values: 3-10\n",
    "#     'max_depth': hp.quniform('max_depth', 7, 23, 1),\n",
    "    \n",
    "#     # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n",
    "#     # (meaning pulling weights to 0). It can be more useful when the objective\n",
    "#     # is logistic regression since you might need help with feature selection.\n",
    "#     'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n",
    "    \n",
    "#     # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n",
    "#     # approach can be more useful in tree-models where zeroing \n",
    "#     # features might not make much sense.\n",
    "#     'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n",
    "    \n",
    "#     # eta: Analogous to learning rate in GBM\n",
    "#     # Makes the model more robust by shrinking the weights on each step\n",
    "#     # Typical final values to be used: 0.01-0.2\n",
    "#     'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    \n",
    "#     # colsample_bytree: Similar to max_features in GBM. Denotes the \n",
    "#     # fraction of columns to be randomly samples for each tree.\n",
    "#     # Typical values: 0.5-1\n",
    "#     'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n",
    "    \n",
    "#     # A node is split only when the resulting split gives a positive\n",
    "#     # reduction in the loss function. Gamma specifies the \n",
    "#     # minimum loss reduction required to make a split.\n",
    "#     # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "#     'gamma': hp.uniform('gamma', 0.01, .7),\n",
    "    \n",
    "#     # more increases accuracy, but may lead to overfitting.\n",
    "#     # num_leaves: the number of leaf nodes to use. Having a large number \n",
    "#     # of leaves will improve accuracy, but will also lead to overfitting.\n",
    "#     'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n",
    "    \n",
    "#     # specifies the minimum samples per leaf node.\n",
    "#     # the minimum number of samples (data) to group into a leaf. \n",
    "#     # The parameter can greatly assist with overfitting: larger sample\n",
    "#     # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n",
    "#     'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n",
    "    \n",
    "#     # subsample: represents a fraction of the rows (observations) to be \n",
    "#     # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n",
    "#     # in their paper A Scalable Tree Boosting System recommend \n",
    "#     'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n",
    "    \n",
    "#     # randomly select a fraction of the features.\n",
    "#     # feature_fraction: controls the subsampling of features used\n",
    "#     # for training (as opposed to subsampling the actual training data in \n",
    "#     # the case of bagging). Smaller fractions reduce overfitting.\n",
    "#     'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n",
    "    \n",
    "#     # randomly bag or subsample training data.\n",
    "#     'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n",
    "    \n",
    "#     # bagging_fraction and bagging_freq: enables bagging (subsampling) \n",
    "#     # of the training data. Both values need to be set for bagging to be used.\n",
    "#     # The frequency controls how often (iteration) bagging is used. Smaller\n",
    "#     # fractions and frequencies reduce overfitting.\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set algoritm parameters\n",
    "# best = fmin(fn=objective,\n",
    "#             space=space,\n",
    "#             algo=tpe.suggest,\n",
    "#             max_evals=27)\n",
    "\n",
    "# # Print best parameters\n",
    "# best_params = space_eval(space, best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"BEST PARAMS: \", best_params)\n",
    "\n",
    "# best_params['max_depth'] = int(best_params['max_depth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning and Predicting with best Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting X test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(-999)\n",
    "X_test = X_test.fillna(-999)\n",
    "clf = xgb.XGBClassifier(n_estimators = 500, \n",
    "                       n_jobs = 4, \n",
    "                       max_depth = 9, \n",
    "                       learning_rate = 0.05, \n",
    "                       sub_sample = 0.9, \n",
    "                       colsample_bytree = 0.9, \n",
    "                       missing = -999)\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_preds = clf.predict_proba(X_test)[:,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 20 Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_important = clf.get_booster().get_score(importance_type=\"weight\")\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n",
    "data = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n",
    "\n",
    "# Top 10 features\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seting y_pred to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['isFraud'] = y_preds\n",
    "sample_submission.to_csv('XGB_hypopt_model_wocardnorm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
